{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"qwertyenv","text":"<p>Gym/Gymnasium environments (Reinforcement Learning)</p> <p>Black Jack (from RLBook2018)</p> <p>Collect Coins (Chess like)</p> <p>\"Ensure Valid Action\" Gym wrapper.</p> <p>\"Up/Down/Left/Right\" Gym wrapper - relevant for example for the Collect Coins environment when the piece is a (simplified) rock.</p> <p>Wrapping a PettingZoo environment into a Gymnasium environment by stating which is the \"external agent\" and by providing a mechanism to fetch the actions for all other agents: \"aec_to_gymnasium\" and \"parallel_to_gymnasium\".</p> <p>pip install qwertyenv</p> <p>Example usages for the Black Jack environment, for the Collect Coins environment, and for the PettingZoo-to-Gymnasium utilities can be found on qwertyenv on github.</p>"},{"location":"black_jack/","title":"BJEnv - BlackJack","text":"<p>This casino-style game is described in RLBook2018. The player needs to decide wheater to ask for an additional card (hits), or to pass the turn to the dealer (sticks). As long as the player takes additional cards and was not busted (sum of cards is still &lt;= 21), the turn stays with the player.  Then the dealer plays (with a fixed rule when to take additional card and when to stop). Finally the player wins when they were not busted, and have a sum that is bigger then the dealer's sum (or the player were not busted yet the dealer was).</p> <p>An Ace can be counted as 1 or as 11. Therefore the first Ace is \"useful\". A second Ace shall always be counted as 1 to avoid &gt; 21 sum (given that the first Ace was counted as 11).</p> <p>The implementation found here in qwertyenv is a Gymnasium environment for the player (you should build a policy for the player).</p> <p>In RLBook2018 this environment is brought in the context of MC with \"Exploring Starts\". Cards are selected not from a deck but rather with randomness from all possibilities (so with repetitions). To the best of my memory this is what is described in the RLBook2018.</p> <p>Below you can see are the action_space and the observation_space.</p> <p>Please note that experimenting MC \"Exploring Starts\" is facilitates as the environment itself contains the uniform randomness for the starting state. We start the game when the player has already at least a sum of 11, as otherwise the only \"smart\" action is 'hits'.</p> part of BJEnv Gymnasium environment<pre><code>self.action_space = gym.spaces.Discrete(2) # 0 hits, 1 sticks\nobs_space = dict(\nplayer_sum = gym.spaces.Discrete(11), # x -&gt; x + 11\nplayer_useful_Ace = gym.spaces.Discrete(2), # 0 no, 1 yes\ndealer_shown_card = gym.spaces.Discrete(10) # 0 - Ace, x -&gt; x + 1 (ex. 1 is 2, 9 is 10)\n)\nself.observation_space = gym.spaces.Dict(obs_space)\n</code></pre>"},{"location":"collect_coins/","title":"CollectCoins","text":"<p>CollectCoins is a Chess like game. Each of the two players, white and black, has one piece, a knight (one can also have rock vs. rock, or also additional combinations). On the board squares there are coins. Each player at its turn move its piece to one of the valid places where the piece can move. If the target square had a coin, the coin is consumed by that player. In this game pieces can not eat one another and cannot stand on the same square. The aim of the game is to collect more coins than the other player, by planning better the path, blocking the opponent, etc.</p> Shown below the initial board state and then the board after both players took their turn<pre><code>---------------------------------\n|wR | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ |bR |\n---------------------------------\n\n0/0\n---------------------------------\n|   |wR | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ | $ | $ |\n---------------------------------\n| $ | $ | $ | $ | $ | $ |bR |   |\n---------------------------------\n\n1/1\n</code></pre> <p>In qwertyenv, at the moment, there are two implementations. One is a Gymnasium environment in which the other player takes a random (legal) move, and another is a PettingZoo environment. Both environments use a common module 'collect_coins_game.py'.</p> <p>When developing and experimenting with this environment, a few relevant wrappers were also developed, such as EnsureValidAction, that overrides your move if it is not legal, UpDownLeftRight that changes the action_space to {Up, Down, Left, Right}.</p> <p>Also developed, and hopefully useful, are two utility functions to wrap (any) PettingZoo environment into the matching Gymnasium environment by declaring which is the \"external agent\" and where to fetch the action for the other agents.</p> <p>The action space for both the Gymnasium and for the PettingZoo environments is the target square. This action spaces was choosen as to unify the action space with various pieces. My experience with this action space is that it is slower to learn than if the action space is specific for a piece (ex. {Up, Down, Left, Right}).</p> <p>The observation space is the board (with the coins presence/absence), and the location of the two pieces.</p> code snippet taken from the CollectCoins Gymnasium environment<pre><code>obs_space = dict(\nboard = gym.spaces.Box(low=0, high=1, shape=(8 * 8,), dtype=bool),\nplayer = gym.spaces.MultiDiscrete([8, 8]),\nother_player = gym.spaces.MultiDiscrete([8, 8]),\n)    \nself.observation_space = gym.spaces.Dict(spaces=obs_space)\nself.action_space = gym.spaces.MultiDiscrete([8, 8])\n</code></pre> <p>To initialize the environment pass the desired pieces (ex. 'rock' and 'rock'). Please note that a 'rock' here is a limited version that can only move one square at a time. You need also to state if you wish to go first or second (player=0 or player=1)</p>"},{"location":"pz_to_gym/","title":"Wrapping a PettingZoo into a Gymnasium one","text":"<p>Two utility functions 'aec_to_gymnasium' and the 'parallel_to_gymnasium' are provided in qwertyenv.</p> <p>In order to have a PettingZoo environment presenting itself as a Gymnasium environment, we need to know which of the agents is the \"external agent\", the one that should not be \"wrapped into\" the Gymnasium environment. In addition we need to know how to get the actions for all other agents.</p> <pre><code>from typing import Any, Callable, Dict, Optional\nimport gymnasium as gym\nfrom gymnasium.core import ActType, ObsType\nfrom pettingzoo import AECEnv, ParallelEnv\n# The first parameter is an agent (its identification),\n#  second parameter is the relevant observation.\n# The callable is expected to return the action that the\n#  given agent would like to take.\nActOthers = Callable[[str, ObsType], ActType]\ndef aec_to_gymnasium(aec_env: AECEnv,\nexternal_agent: str, act_others: ActOthers):\n\"\"\"Makes a Gymnasium environment out of a AECEnv.\n    ...\n    \"\"\"\n...\ndef parallel_to_gymnasium(parallel_env: ParallelEnv,\nexternal_agent: str, act_others: ActOthers):\n\"\"\"Makes a Gymnasium environment out of a ParallelEnv.\n    ...\n    \"\"\"\n...\n</code></pre> <p>Below is a brief description of the implementation. For both functions, I return an instance of a (inner) class defined in the function itself <code>class WrapperEnv(gym.Env)</code>.  'aec_to_gymnasium' was a bit trickier. I had to check if the current agent is the 'external_agent' or is it another agent. I used the following member function of my 'WrapperEnv' class.</p> part of (inner) class WrapperEnv(gym.Env) - aec_to_gymnasium<pre><code>def _loop_others(self):\nfor agent in self._aec_env.agent_iter():\nif agent == self._external_agent:\nbreak\nobservation, _, terminated, truncated, _ = self._aec_env.last()\nif terminated or truncated:\nbreak\naction_current = self._act_others(agent, observation)\nself._aec_env.step(action_current)\n</code></pre> <p>For 'parallel_to_gymnasium' it was just a question of where to take the next action, from the provided argument to the 'step' method, or from 'act_others' callable that was provieded in the initialization.</p> part of (inner) class WrapperEnv(gym.Env) - parallel_to_gymnasium<pre><code>def step(self, action):\n...\nactions = {\nagent: (\naction\nif agent == self._external_agent\nelse self._act_others(agent, self._observations[agent])\n)\nfor agent in self._parallel_env.agents\n}\n...\n</code></pre> <p>An example for the usage of those functions, is as follows:</p> <pre><code>def test_tictactoe(me, other):\naec_env = tictactoe_v3.env()\ndef pick_a_free_square(obs):\naction_mask = obs[\"action_mask\"]\npossible_actions = np.where(action_mask == 1)[0]\nreturn np.random.choice(possible_actions)\nother_agents_logic = {other: pick_a_free_square}\ngym_env = aec_to_gymnasium(\naec_env=aec_env,\nexternal_agent=me,\nact_others=(\nlambda agent, observation: other_agents_logic[agent](observation)\n),\n)\n...\n</code></pre> <p>In above example, the lambda function 'act_others' is making a use of a dict 'other_agents_logic'. In this simple case, we could also just ignore the 'agent' argument, and call 'pick_a_free_square' directly.</p>"}]}